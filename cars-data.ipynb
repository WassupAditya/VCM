{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import models , layers\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport shutil\n\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set the constaant values\nbatch_size = 32\nimage_size = 256\nchannels = 3\nepochs = 10\ndataset = tf.keras.utils.image_dataset_from_directory(directory=\"/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train\",batch_size = batch_size , image_size = (image_size,image_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = dataset.class_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset_split(ds,train_split=0.7,val_split=0.15,test_split=0.15,shuffle_size=10000):\n    ds_size = len(ds) #df_size should be equal to the size of data\n    if shuffle_size:\n        ds=ds.shuffle(shuffle_size,seed=42) #shuffle the data randomly\n        train_size = int(train_split*ds_size)\n        val_size = int(val_split*ds_size)\n        train_ds = ds.take(train_size)\n        val_ds = ds.skip(train_size).take(val_size)\n        test_ds = ds.skip(train_size).skip(val_size)\n        print(f\"Train size : {train_ds}\")\n        print(f\"Test size : {test_ds}\")\n        print(f\"Validation size : {val_ds}\")\n        return train_ds,val_ds,test_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds , validation_ds ,test_ds=dataset_split(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the data\nplt.figure(figsize=(15,15))\nfor image_batch,label_batch in dataset.take(1):\n    for i in range(12):\n        ax=plt.subplot(3,4,i+1)\n        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\n        plt.title(class_names[label_batch[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_ds=validation_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_ds=test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_and_rescale = tf.keras.Sequential([\n    layers.experimental.preprocessing.Resizing(image_size,image_size), #convert each image to 255,255\n    layers.experimental.preprocessing.Rescaling(1./255) # Normalize each image to 0-1\n])\n\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n])\ninput_size = (batch_size,image_size,image_size,channels) #(32,256,256,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = len(class_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nmodel = tf.keras.models.Sequential([\n    data_augmentation,\n    layers.Conv2D(16,(3,3),activation=\"relu\",input_shape=input_size),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(32,(3,3),activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(196,activation=\"relu\"),\n    layers.Dense(1,activation=\"softmax\")\n])\nmodel.build(input_shape=input_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss=tf.keras.losses.categorical_crossentropy,metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_ds,epochs=epochs,batch_size=batch_size,verbose=1,validation_data=val_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Matplotlib\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nval_loss=history.history['val_loss']\nloss=history.history['loss']\n\nplt.figure(figsize=(8,4))\nplt.plot(range(epochs),acc,label=\"Training Acc\")\nplt.plot(range(epochs),val_acc,label=\"Val Acc\")\nplt.legend(loc=\"lower right\")\nplt.title('Training and Validation Accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(range(epochs),loss,label=\"Training loss\")\nplt.plot(range(epochs),val_loss,label=\"Val loss\")\nplt.legend(loc=\"lower left\")\nplt.title('Training and Validation loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}